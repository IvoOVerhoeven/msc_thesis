data:
  language: English
  treebank_name: pud
  batch_first: True
  len_sorted: True
  batch_size: 1

model:
  transformer_type: "distilbert"
  transformer_name: "distilbert-base-multilingual-cased"
  transformer_dropout: 0.25
  c2w_kwargs:
      embedding_dim: 256
      h_dim: 384
      bidirectional: True
      out_dim: -1
      batch_first: True
      dropout: 0.25
  layer_attn_kwargs:
    L: 6
    dropout: 0.2
  rnn_kwargs:
      input_size: 768
      h_dim: 768
      num_layers: 2
      residual: True
      dropout: 0.5
      rnn_type: "lstm"
      batch_first: True
      bidirectional: True
  label_smoothing: 0.03
  mask_p: 0.25
  transformer_lrs:
      0: 1.0e-5
      1: 1.0e-5
      2: 1.0e-5
      3: 5.0e-5
      4: 5.0e-5
      5: 5.0e-5
  rnn_lr: 1.0e-3
  clf_lr: 4.0e-3
  n_warmup_steps: 8000
  optim_kwargs:
    betas:
      - 0.9
      - 0.99
    weight_decay: 1.0e-2
  unfreeze_transformer_epoch: 1


trainer:
  gradient_clip_val: 2
  max_epochs: 50
  precision: 16
  num_sanity_val_steps: 0

logging:
  logger: wandb
  logger_kwargs:
    log_model: False
    offline: True
  monitor: valid_total_loss
  monitor_mode: 'min'

run:
  experiment_name: UDIFY_mono
  seed: 610
  gpu: 1
  deterministic: False
  debug: False
  fdev_run: False
  prog_bar_refresh_rate: 100
