batch_first: True
transformer_dropout: 0.2
embedding_dropout: 0.5
mha_kwargs:
  n_heads: 16
  dropout: 0.2
rnn_kwargs:
  num_layers: 3
  residual: True
  dropout: 0.5
  rnn_type: "lstm"
  bidirectional: True
label_smoothing: 0.03
mask_p: 0.0
transformer_lrs: null
#  char_embeddings: 1.0e-5
#  initial_char_encoder: 1.0e-5
#  chars_to_molecules: 1.0e-5
#  encoder: 5.0e-5
#  projection: 1.0e-4
#  final_char_encoder: 1.0e-4
#  pooler: 1.0e-4
rnn_lr: 1.0e-3
clf_lr: 2.5e-3
n_warmup_steps: 0.04
optim_kwargs:
  betas:
    - 0.9
    - 0.99
  weight_decay: 1.0e-2
unfreeze_transformer_epoch: 1
