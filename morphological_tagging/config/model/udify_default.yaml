c2w_kwargs:
  embedding_dim: 256
  h_dim: 384
  bidirectional: True
  out_dim: -1
  batch_first: True
  dropout: 0.5
char_mask_p: 0.05
token_embeddings_dropout: 0.5
rnn_kwargs:
  input_size: 768
  h_dim: 768
  num_layers: 2
  residual: True
  dropout: 0.5
  rnn_type: "lstm"
  batch_first: True
  bidirectional: True
label_smoothing: 0.03
mask_p: 0.25
rnn_lr: 1.0e-3
clf_lr: 4.0e-3
n_warmup_steps: 0.04
optim_kwargs:
  betas:
    - 0.9
    - 0.99
  weight_decay: 1.0e-2
unfreeze_transformer_epoch: 1
layer_attn_kwargs:
  dropout: 0.2
